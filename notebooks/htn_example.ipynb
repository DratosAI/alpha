{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import cloudpickle\n",
    "import pickle\n",
    "import collections\n",
    "import ray\n",
    "import threading\n",
    "import hmac\n",
    "\n",
    "def serialize_sqlite_connection(conn):\n",
    "    return ray.data.datasource\n",
    "\n",
    "def deserialize_sqlite_connection(path):\n",
    "    return sqlite3.connect(path)\n",
    "\n",
    "def serialize_thread_lock(lock):\n",
    "    print(\"serialized_thread_lock\")\n",
    "    return None\n",
    "\n",
    "def deserialize_thread_lock(_):\n",
    "    return None\n",
    "\n",
    "class SerializableHMAC:\n",
    "    def __init__(self, hmac_obj):\n",
    "        self.msg = hmac_obj.digest()\n",
    "        self.digestmod = hmac_obj.digest_size\n",
    "\n",
    "    @classmethod\n",
    "    def from_hmac(cls, hmac_obj):\n",
    "        return cls(hmac_obj)\n",
    "\n",
    "    def to_hmac(self):\n",
    "        return hmac.new(b'', self.msg, digestmod=self.digestmod)\n",
    "\n",
    "def serialize_hmac(hmac_obj):\n",
    "    return SerializableHMAC.from_hmac(hmac_obj)\n",
    "\n",
    "def deserialize_hmac(serialized_hmac):\n",
    "    return serialized_hmac.to_hmac()\n",
    "\n",
    "class HandleWrapper:\n",
    "    def __init__(self, obj):\n",
    "        self.class_name = type(obj).__name__\n",
    "        self.attributes = {}\n",
    "        for key, value in obj.__dict__.items():\n",
    "            if not key.startswith('_'):\n",
    "                try:\n",
    "                    cloudpickle.dumps(value)\n",
    "                    self.attributes[key] = value\n",
    "                except:\n",
    "                    self.attributes[key] = f\"Unpicklable_{type(value).__name__}\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_object(cls, obj):\n",
    "        return cls(obj)\n",
    "\n",
    "    def to_object(self):\n",
    "        # This is a placeholder. You might need to implement proper reconstruction logic.\n",
    "        return type(self.class_name, (), self.attributes)()\n",
    "\n",
    "def serialize_handle_object(obj):\n",
    "    return HandleWrapper.from_object(obj)\n",
    "\n",
    "def deserialize_handle_object(wrapped):\n",
    "    return wrapped.to_object()\n",
    "\n",
    "\n",
    "# # Register the custom serializers with Ray\n",
    "# ray.util.register_serializer(\n",
    "#     object,  # This will catch all objects\n",
    "#     serializer=lambda obj: serialize_handle_object(obj) if hasattr(obj, 'handle') or not cloudpickle.is_picklable(obj) else obj,\n",
    "#     deserializer=lambda obj: deserialize_handle_object(obj) if isinstance(obj, HandleWrapper) else obj\n",
    "# )\n",
    "\n",
    "ray.util.register_serializer(sqlite3.Connection, serializer=serialize_sqlite_connection, deserializer=deserialize_sqlite_connection)\n",
    "ray.util.register_serializer(type(threading.Lock), serializer=serialize_thread_lock, deserializer=deserialize_thread_lock)\n",
    "ray.util.register_serializer(hmac.HMAC, serializer=serialize_hmac, deserializer=deserialize_hmac)\n",
    "\n",
    "# Initialize Ray\n",
    "if not ray.is_initialized():\n",
    "    ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serialized_thread_lock\n",
      "serialized_thread_lock\n",
      "serialized_thread_lock\n",
      "serialized_thread_lock\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not serialize the function 1909578674.htn_workflow:\n=====================================================================\nChecking Serializability of <function htn_workflow at 0x7f5e35edc8b0>\n=====================================================================\n\u001b[31m!!! FAIL\u001b[39m serialization: cannot pickle '_thread.lock' object\nDetected 3 global variables. Checking serializability...\n    Serializing 'ray' <module 'ray' from '/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/__init__.py'>...\n    Serializing 'process_graph' <ray.remote_function.RemoteFunction object at 0x7f5e36828700>...\n    \u001b[31m!!! FAIL\u001b[39m serialization: cannot pickle '_thread.lock' object\n        Serializing '_function' <function process_graph at 0x7f5e35edd2d0>...\n        \u001b[31m!!! FAIL\u001b[39m serialization: cannot pickle '_thread.lock' object\n        Detected 1 global variables. Checking serializability...\n            Serializing 'GraphAgent' <class '__main__.GraphAgent'>...\n            \u001b[31m!!! FAIL\u001b[39m serialization: cannot pickle '_thread.lock' object\n        Serializing '__generator_backpressure_num_objects' None...\n=====================================================================\nVariable: \n\n\t\u001b[1mFailTuple(GraphAgent [obj=<class '__main__.GraphAgent'>, parent=<function process_graph at 0x7f5e35edd2d0>])\u001b[0m\n\nwas found to be non-serializable. There may be multiple other undetected variables that were non-serializable. \nConsider either removing the instantiation/imports of these variables or moving the instantiation into the scope of the function/class. \n=====================================================================\nCheck https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting for more information.\nIf you have any suggestions on how to improve this error message, please reach out to the Ray developers on github.com/ray-project/ray/issues/\n=====================================================================\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/_private/serialization.py:67\u001b[0m, in \u001b[0;36mpickle_dumps\u001b[0;34m(obj, error_msg)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/cloudpickle/cloudpickle.py:1479\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m   1478\u001b[0m cp \u001b[38;5;241m=\u001b[39m Pickler(file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback)\n\u001b[0;32m-> 1479\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/cloudpickle/cloudpickle.py:1245\u001b[0m, in \u001b[0;36mPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 104\u001b[0m\n\u001b[1;32m     96\u001b[0m relations \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     97\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelates_to\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mart1\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     98\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelates_to\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mart2\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     99\u001b[0m ]\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Run the workflow\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# mlflow.set_experiment(\"HTN_Agent_System\")\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# with mlflow.start_run(run_name=\"HTN_Workflow\"):\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m htn, processed_docs, processed_artifacts, graph \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mget(\u001b[43mhtn_workflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifacts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelations\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Log metrics and parameters\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# mlflow.log_metric(\"num_documents\", len(processed_docs))\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# mlflow.log_metric(\"num_artifacts\", len(processed_artifacts))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# mlflow.log_dict(htn, \"htn.json\")\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated HTN:\u001b[39m\u001b[38;5;124m\"\u001b[39m, htn)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/remote_function.py:139\u001b[0m, in \u001b[0;36mRemoteFunction.__init__.<locals>._remote_proxy\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_remote_proxy\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remote\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py:310\u001b[0m, in \u001b[0;36m_tracing_task_invocation.<locals>._invocation_remote_span\u001b[0;34m(self, args, kwargs, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ray_trace_ctx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ray_trace_ctx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[1;32m    313\u001b[0m tracer \u001b[38;5;241m=\u001b[39m _opentelemetry\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mget_tracer(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/remote_function.py:304\u001b[0m, in \u001b[0;36mRemoteFunction._remote\u001b[0;34m(self, args, kwargs, **task_options)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_descriptor \u001b[38;5;241m=\u001b[39m PythonFunctionDescriptor\u001b[38;5;241m.\u001b[39mfrom_function(\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uuid\n\u001b[1;32m    294\u001b[0m )\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# There is an interesting question here. If the remote function is\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# used by a subsequent driver (in the same script), should the\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# second driver pickle the function again? If yes, then the remote\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# first driver. This is an argument for repickling the function,\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# which we do here.\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickled_function \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_dumps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCould not serialize the function \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepr\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_export_cluster_and_job \u001b[38;5;241m=\u001b[39m worker\u001b[38;5;241m.\u001b[39mcurrent_cluster_and_job\n\u001b[1;32m    310\u001b[0m worker\u001b[38;5;241m.\u001b[39mfunction_actor_manager\u001b[38;5;241m.\u001b[39mexport(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/_private/serialization.py:72\u001b[0m, in \u001b[0;36mpickle_dumps\u001b[0;34m(obj, error_msg)\u001b[0m\n\u001b[1;32m     70\u001b[0m inspect_serializability(obj, print_file\u001b[38;5;241m=\u001b[39msio)\n\u001b[1;32m     71\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msio\u001b[38;5;241m.\u001b[39mgetvalue()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not serialize the function 1909578674.htn_workflow:\n=====================================================================\nChecking Serializability of <function htn_workflow at 0x7f5e35edc8b0>\n=====================================================================\n\u001b[31m!!! FAIL\u001b[39m serialization: cannot pickle '_thread.lock' object\nDetected 3 global variables. Checking serializability...\n    Serializing 'ray' <module 'ray' from '/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/__init__.py'>...\n    Serializing 'process_graph' <ray.remote_function.RemoteFunction object at 0x7f5e36828700>...\n    \u001b[31m!!! FAIL\u001b[39m serialization: cannot pickle '_thread.lock' object\n        Serializing '_function' <function process_graph at 0x7f5e35edd2d0>...\n        \u001b[31m!!! FAIL\u001b[39m serialization: cannot pickle '_thread.lock' object\n        Detected 1 global variables. Checking serializability...\n            Serializing 'GraphAgent' <class '__main__.GraphAgent'>...\n            \u001b[31m!!! FAIL\u001b[39m serialization: cannot pickle '_thread.lock' object\n        Serializing '__generator_backpressure_num_objects' None...\n=====================================================================\nVariable: \n\n\t\u001b[1mFailTuple(GraphAgent [obj=<class '__main__.GraphAgent'>, parent=<function process_graph at 0x7f5e35edd2d0>])\u001b[0m\n\nwas found to be non-serializable. There may be multiple other undetected variables that were non-serializable. \nConsider either removing the instantiation/imports of these variables or moving the instantiation into the scope of the function/class. \n=====================================================================\nCheck https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting for more information.\nIf you have any suggestions on how to improve this error message, please reach out to the Ray developers on github.com/ray-project/ray/issues/\n=====================================================================\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, Any, List\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# Initialize Ray\n",
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "\n",
    "# Define models (not used directly with Ray)\n",
    "class DataObject(BaseModel):\n",
    "    content: Dict[str, Any]\n",
    "    metadata: Dict[str, Any] = {}\n",
    "    vector: List[float] = None\n",
    "\n",
    "class Artifact(BaseModel):\n",
    "    artifact_id: str\n",
    "    payload: bytes\n",
    "    metadata: Dict[str, Any] = {}\n",
    "\n",
    "class Agent(BaseModel):\n",
    "    agent_id: str\n",
    "\n",
    "    def process(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "\n",
    "class DocumentAgent(Agent):\n",
    "    def process(self, data_object: Dict[str, Any]):\n",
    "        data_object['metadata']['length'] = len(str(data_object['content']))\n",
    "        data_object['metadata']['type'] = type(data_object['content']).__name__\n",
    "        return data_object\n",
    "\n",
    "class ArtifactAgent(Agent):\n",
    "    def process(self, artifact: Dict[str, Any]):\n",
    "        print(f\"Saving artifact {artifact['artifact_id']} to storage\")\n",
    "        return artifact\n",
    "\n",
    "class GraphAgent(Agent):\n",
    "    def process(self, relations: List[Dict[str, str]]):\n",
    "        return pd.DataFrame(relations).to_dict()\n",
    "\n",
    "class HTNGenerator(Agent):\n",
    "    def process(self, data_objects: List[Dict], artifacts: List[Dict], graph: Dict):\n",
    "        return {\n",
    "            \"root\": \"process_data\",\n",
    "            \"subtasks\": [\n",
    "                {\"task\": \"process_documents\", \"objects\": data_objects},\n",
    "                {\"task\": \"process_artifacts\", \"objects\": artifacts},\n",
    "                {\"task\": \"analyze_graph\", \"relations\": graph}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Ray remote functions\n",
    "@ray.remote\n",
    "def process_document(data_object: Dict[str, Any]):\n",
    "    agent = DocumentAgent(agent_id=\"doc_agent\").model_dump()\n",
    "    return agent.process(data_object)\n",
    "\n",
    "@ray.remote\n",
    "def process_artifact(artifact: Dict[str, Any]):\n",
    "    agent = ArtifactAgent(agent_id=\"artifact_agent\").model_dump()\n",
    "    return agent.process(artifact)\n",
    "\n",
    "@ray.remote\n",
    "def process_graph(relations: List[Dict[str, str]]):\n",
    "    agent = GraphAgent(agent_id=\"graph_agent\")\n",
    "    return agent.process(relations)\n",
    "\n",
    "@ray.remote\n",
    "def generate_htn(data_objects: List[Dict], artifacts: List[Dict], graph: Dict):\n",
    "    agent = HTNGenerator(agent_id=\"htn_generator\")\n",
    "    return agent.process(data_objects, artifacts, graph)\n",
    "\n",
    "@ray.remote\n",
    "def htn_workflow(data_objects: List[Dict], artifacts: List[Dict], relations: List[Dict[str, str]]):\n",
    "    processed_docs = ray.get([process_document.remote(obj) for obj in data_objects])\n",
    "    processed_artifacts = ray.get([process_artifact.remote(art) for art in artifacts])\n",
    "    graph = ray.get(process_graph.remote(relations))\n",
    "    htn = ray.get(generate_htn.remote(processed_docs, processed_artifacts, graph))\n",
    "    return htn, processed_docs, processed_artifacts, graph\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample data (as dictionaries)\n",
    "    data_objects = [\n",
    "        {\"content\": {\"text\": \"Sample document 1\"}},\n",
    "        {\"content\": {\"text\": \"Sample document 2\"}}\n",
    "    ]\n",
    "    artifacts = [\n",
    "        {\"artifact_id\": str(uuid.uuid4()), \"payload\": b\"Sample binary data 1\"},\n",
    "        {\"artifact_id\": str(uuid.uuid4()), \"payload\": b\"Sample binary data 2\"}\n",
    "    ]\n",
    "    relations = [\n",
    "        {\"subject\": \"doc1\", \"predicate\": \"relates_to\", \"object\": \"art1\"},\n",
    "        {\"subject\": \"doc2\", \"predicate\": \"relates_to\", \"object\": \"art2\"}\n",
    "    ]\n",
    "\n",
    "    # Run the workflow\n",
    "    # mlflow.set_experiment(\"HTN_Agent_System\")\n",
    "    # with mlflow.start_run(run_name=\"HTN_Workflow\"):\n",
    "    htn, processed_docs, processed_artifacts, graph = ray.get(htn_workflow.remote(data_objects, artifacts, relations))\n",
    "        \n",
    "        # Log metrics and parameters\n",
    "        # mlflow.log_metric(\"num_documents\", len(processed_docs))\n",
    "        # mlflow.log_metric(\"num_artifacts\", len(processed_artifacts))\n",
    "        # mlflow.log_metric(\"num_relations\", len(graph))\n",
    "        \n",
    "        # for i, doc in enumerate(processed_docs):\n",
    "        #     mlflow.log_metric(f\"doc_{i}_length\", doc['metadata']['length'])\n",
    "        \n",
    "        # for i, art in enumerate(processed_artifacts):\n",
    "        #     mlflow.log_param(f\"artifact_{i}_id\", art['artifact_id'])\n",
    "        \n",
    "        # mlflow.log_dict(htn, \"htn.json\")\n",
    "\n",
    "    print(\"Generated HTN:\", htn)\n",
    "\n",
    "    # Query MLflow for results\n",
    "    print(\"\\nMLflow Runs:\")\n",
    "    for run in mlflow.search_runs():\n",
    "        print(f\"Run ID: {run.run_id}\")\n",
    "        print(f\"Metrics: {run.data.metrics}\")\n",
    "        print(f\"Params: {run.data.params}\")\n",
    "        print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
